{
    "collab_server" : "",
    "contents" : "# seperating into two kinds outliers and normal data ----\nT1_traindata = subset(traindata, UUL_flag == 1 | DR_flag == 1)\nT1_testdata = subset(testdata, UUL_flag == 1 | DR_flag == 1)\n\n## T1 data modeling\n# Missing data checking ----\nMissing_data_Check(T1_traindata)\n\n# Splitting the training data (SET1: build and tune the model) ~ (SET1: test the model) ----\nset.seed(123)\nsplit = sample.split(T1_traindata$Dependents, SplitRatio = 0.75)\nT1_traindata_Train = subset(T1_traindata, split == TRUE)\nT1_traindata_Test = subset(T1_traindata, split == FALSE)\n\n# Missing data handling ----\nT1_traindata_Test = Missing_data_handling(T1_traindata_Test)\nMissing_data_Check(T1_traindata_Test)\nT1_traindata_Train = Missing_data_handling(T1_traindata_Train)\nMissing_data_Check(T1_traindata_Train)\n# Target variable ratio check ----\nTarget_Ratio_Check(T1_traindata_Test) #93:7 \nTarget_Ratio_Check(T1_traindata_Train) #92:8\n# SMOTE for treating imbalance data set ----\nTwo_D_View(T1_traindata_Test)\nT1_traindata_Test_SMOTEd = SMOTE_fitting(T1_traindata_Test,600,300) #72:28\nTwo_D_View(T1_traindata_Train)\nT1_traindata_Train_SMOTEd = SMOTE_fitting(T1_traindata_Train,600,300) #72:28\n\n# SMOTE has oversampled the major class area too - so trying boundary SMOTE ----\nTwo_D_View(T1_traindata_Test)\nT1_traindata_Test_BS = Boderline_SMOTE_fitting(T1_traindata_Test,8) #73:27\n\nTwo_D_View(T1_traindata_Train)\nT1_traindata_Train_BS = Boderline_SMOTE_fitting(T1_traindata_Train,15) #71:29\nrm(list = c('T1_traindata_Test_SMOTEd','T1_traindata_Train_SMOTEd')) # Removing as SMOTE has overfitted the majored regions too\n\n# Building a Scaled data set for classification models ----\nT1_traindata_Test_BS_Scaled = Scaling(T1_traindata_Test_BS)\nT1_traindata_Train_BS_Scaled = Scaling(T1_traindata_Train_BS)\n# Logistic regression -- Specificity: Train - 72.88 K-fold Train - 69.27 Test - 80.58 ----\nT1_LR = glm( formula = DLQs~., \n             family = binomial,\n             data = T1_traindata_Train_BS_Scaled)\nprob_pred = predict(T1_LR, type = 'response', newdata = T1_traindata_Train_BS_Scaled[-1])\ny_pred = ifelse(prob_pred > 0.4, 1, 0)\nCM = table(T1_traindata_Train_BS_Scaled[,1],y_pred)\nLR_Speci_Train = CM[4]/(CM[4]+CM[2])\nround(LR_Speci_Train*100,2)\nLR_Acc = (CM[4]+CM[1])/(CM[4]+CM[1]+CM[2]+CM[3])\nLR_Acc\n\nrequire(lmtest)\nlrtest(T1_LR) # overall test i significant\nrequire(pscl)\npR2(T1_LR) # 18 - ok model by McFadden R2\n\nset.seed(1234)\nfolds = createFolds(T1_traindata_Train_BS_Scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = T1_traindata_Train_BS_Scaled[-x, ]\n  test_fold = T1_traindata_Train_BS_Scaled[x, ]\n  T1_LR_KF = glm( formula = DLQs~., \n                  family = binomial,\n                  data = training_fold)\n  prob_pred = predict(T1_LR_KF, type = 'response', newdata = test_fold[-1])\n  y_pred = ifelse(prob_pred > 0.4, 1, 0)\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nLR_Speci_KF = mean(as.numeric(cv))\nround(LR_Speci_KF*100,2)\n\nprob_pred = predict(T1_LR, type = 'response', newdata = T1_traindata_Test_BS_Scaled[-1])\ny_pred = ifelse(prob_pred > 0.4, 1, 0)\nCM = table(T1_traindata_Test_BS_Scaled[,1],y_pred)\nLR_Speci_Test = CM[4]/(CM[4]+CM[2])\nround(LR_Speci_Test*100,2)\n\n# KNN Classification -- Specificity:Train - xxxxx K-fold Train - 71.25 Test 41.73  ---- \ncaret_tune = train(form = DLQs~ ., data = T1_traindata_Train_BS_Scaled, method = 'knn')\ncaret_tune\ncaret_tune$bestTune # caret to tune for k value\n\ny_pred = knn(train =T1_traindata_Train_BS_Scaled[,-1],\n             test =T1_traindata_Test_BS_Scaled[,-1],\n             cl = T1_traindata_Train_BS_Scaled[, 1],\n             k = 5,\n             prob = TRUE)\nCM = table(T1_traindata_Test_BS_Scaled[,1],y_pred)\nKnn_Speci_Test = CM[4]/(CM[4]+CM[2])\nKnn_Speci_Test\n\nset.seed(1234)\nfolds = createFolds(T1_traindata_Train_BS_Scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = T1_traindata_Train_BS_Scaled[-x, ]\n  test_fold = T1_traindata_Train_BS_Scaled[x, ]\n  y_pred = knn(train =training_fold[,-1],\n               test =test_fold[,-1],\n               cl = training_fold[, 1],\n               k = 5,\n               prob = TRUE)\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nKnn_Speci_KF = mean(as.numeric(cv)) #overfitted\n\n# SVM Classification -- Specificity:Train - 63.4  K-fold Train - 59.097 Test 46.76  ---- \nset.seed(1234)\ncaret_tune = train(form = DLQs~ ., data = T1_traindata_Train_BS_Scaled, method = 'svmLinearWeights')\ncaret_tune\ncaret_tune$bestTune # caret to tune for cost and weight value - cost is 1 which is default\nset.seed(1234)\ntune_svm_kernal = tune(svm, DLQs~ ., data = T1_traindata_Train_BS_Scaled,\n                       kernal = 'radial',\n                       ranges = list(cost = c(0.1,0.4,0.8,1,3,5,10,50,100), # penalising factor for missclassification, high c => low bias, high viariance, default is 1\n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4))) # smoothening the boundary shape sharpness, low gama => pointy bounday, low bias, high variance, default 1/dimensions\nsummary(tune_svm_kernal) # tuned parameters says cost 50 and gamma 0.9\nset.seed(1234)\ntune_svm_kernal = tune(svm, DLQs~ ., data = T1_traindata_Train_BS_Scaled,\n                       kernal = 'sigmoid',\n                       ranges = list(cost = c(0.1,0.4,0.8,1,3,5,10,50,100), \n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4))) \nsummary(tune_svm_kernal) # tuned parameters says cost 50 and gamma 0.9\nset.seed(1234)\ntune_svm_kernal = tune(svm, DLQs~ ., data = T1_traindata_Train_BS_Scaled,\n                       kernal = 'polynomial',\n                       ranges = list(ccost = c(0.1,0.4,0.8,1,3,5,10,50,100),\n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4),\n                                     degree = c(2,3,4,5,6)))\nsummary(tune_svm_kernal) # tuned parameters says cost 0.1  and gamma 4 and degree 2\n\nfor(svmType in c('C-classification','nu-classification')){\n  for(svmKernal in c('linear','polynomial','radial','sigmoid')){\n    set.seed(1234)\n    folds = createFolds(T1_traindata_Train_BS_Scaled$DLQs, k = 10)\n    cv = lapply(folds, function(x) {\n      training_fold = T1_traindata_Train_BS_Scaled[-x, ]\n      test_fold = T1_traindata_Train_BS_Scaled[x, ]\n      if(svmKernal == 'radial'){\n        T1_SVM = svm(formula = DLQs ~ .,\n                     data = training_fold,\n                     type = 'C-classification',\n                     kernel = svmKernal, cost = 50,gamma = 0.9)\n        y_pred = predict(T1_SVM, newdata = test_fold[-1])\n      }else if(svmKernal=='sigmoid'){\n        T1_SVM = svm(formula = DLQs ~ .,\n                     data = training_fold,\n                     type = 'C-classification',\n                     kernel = svmKernal, cost = 50 ,gamma = 0.9)\n        y_pred = predict(T1_SVM, newdata = test_fold[-1])\n      }else if(svmKernal=='polynomial'){\n        T1_SVM = svm(formula = DLQs ~ .,\n                     data = training_fold,\n                     type = 'C-classification',\n                     kernel = svmKernal, cost =0.1 ,gamma = 4 ,degre = 2)\n        y_pred = predict(T1_SVM, newdata = test_fold[-1])\n      }else{\n        T1_SVM = svm(formula = DLQs ~ .,\n                     data = training_fold,\n                     type = 'C-classification',\n                     kernel = svmKernal, cost =1)\n        y_pred = predict(T1_SVM, newdata = test_fold[-1])\n      }\n      CM = table(test_fold[,1],y_pred)\n      temp = CM[4]/(CM[4]+CM[2])\n      return(temp)\n    })\n    specificity_SVM = round(mean(as.numeric(cv)),5)*100\n    print.noquote(paste0(svmKernal,'-kernal ',svmType,' has K-fold specificity of ',specificity_SVM))\n  }\n} # choose radial kernal with C-Classification as it has highest 59.097\n\nT1_SVM = svm(formula = DLQs ~ .,\n             data = T1_traindata_Train_BS_Scaled,\n             type = 'C-classification',\n             kernel = 'radial', cost= 50, gamma= 9)\ny_pred = predict(T1_SVM, newdata = T1_traindata_Train_BS_Scaled[-1])\nCM = table(T1_traindata_Train_BS_Scaled[,1],y_pred)\nSVM_Speci_Train = CM[4]/(CM[4]+CM[2])\n\ny_pred = predict(T1_SVM, newdata = T1_traindata_Test_BS_Scaled[-1])\nCM = table(T1_traindata_Test_BS_Scaled[,1],y_pred)\nSVM_Speci_Test = CM[4]/(CM[4]+CM[2]) # dropped in Test, overfitted, but will consider for Test set, as k-fold is close to train_test\n\n# Naive Bayes -- Specificity:Train - 91.83 K-fold Train - 81.785 Test 5.7   ----\nT1_NB = naiveBayes(x = T1_traindata_Train_BS_Scaled[-1],\n                   y = T1_traindata_Train_BS_Scaled$DLQs)\n\ny_pred = predict(T1_NB, newdata = T1_traindata_Train_BS_Scaled[-1])\nCM = table(T1_traindata_Train_BS_Scaled[,1],y_pred)\nNB_Speci_Train = CM[4]/(CM[4]+CM[2])\n\nset.seed(1234)\nfolds = createFolds(T1_traindata_Train_BS_Scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = T1_traindata_Train_BS_Scaled[-x, ]\n  test_fold = T1_traindata_Train_BS_Scaled[x, ]\n  T1_NB = naiveBayes(x = training_fold[-1],\n                     y = training_fold$DLQs)\n  y_pred = predict(T1_NB, newdata = test_fold[-1])\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nNB_Speci_KF = round(mean(as.numeric(cv)),5)*100 \n\ny_pred = predict(T1_NB, newdata = T1_traindata_Test_BS_Scaled[-1])\nCM = table(T1_traindata_Test_BS_Scaled[,1],y_pred)\nNB_Speci_Test = CM[4]/(CM[4]+CM[2]) # overfitted\n\n# CART -- Specificity:Train - 81.67 K-fold Train - 77.441  Test 77.7    --------\ncaret_tune = train(form = DLQs~ ., data = T1_traindata_Train_BS_Scaled, method = 'rpart')\ncaret_tune\ncaret_tune$bestTune # CP - Tunning 0.06535947712\n\nT1_CART_temp = rpart(formula = DLQs ~ ., \n                     data = T1_traindata_Train_BS_Scaled, \n                     method = \"class\", \n                     minsplit= 25, \n                     cp = 0, \n                     xval = 7)\nprintcp(T1_CART_temp)\nplotcp(T1_CART_temp)\nT1_CART = prune(T1_CART_temp, cp= 0.06535947712 ,\"CP\")\ny_pred = predict(T1_CART, newdata = T1_traindata_Train_BS_Scaled[-1], type='class')\nCM = table(T1_traindata_Train_BS_Scaled[,1],y_pred)\nCART_Speci_Train = CM[4]/(CM[4]+CM[2])\n\nset.seed(1234)\nfolds = createFolds(T1_traindata_Train_BS_Scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = T1_traindata_Train_BS_Scaled[-x, ]\n  test_fold = T1_traindata_Train_BS_Scaled[x, ]\n  T1_CART_temp = rpart(formula = DLQs ~ ., \n                       data = training_fold, \n                       method = \"class\", \n                       minsplit= 25, \n                       cp = 0.06535947712, \n                       xval = 7)\n  y_pred = predict(T1_CART_temp, newdata = test_fold[-1], type='class')\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nCART_Speci_KF = round(mean(as.numeric(cv)),5)*100\n\ny_pred = predict(T1_CART, newdata = T1_traindata_Test_BS_Scaled[-1],type='class')\nCM = table(T1_traindata_Test_BS_Scaled[,1],y_pred)\nCART_Speci_Test = CM[4]/(CM[4]+CM[2]) # moves to build the test solution\n\n# Random Forest -- Specificity:Train - 88.8  K-fold Train - 83.67 Test 19.42 ------\nset.seed(1234)\nT1_RF = randomForest(DLQs ~ ., data = T1_traindata_Train_BS_Scaled, \n                     ntree=500, mtry = 2, nodesize = 20,\n                     importance=TRUE)\nplot(T1_RF) ## 150 tree from OOB\ncaret_tune = train(form = DLQs~ ., data = T1_traindata_Train_BS_Scaled, method = 'rf')\ncaret_tune\ncaret_tune$bestTune # mtry - Tunning \n\nset.seed(1234)\nT1_RF = tuneRF(x = T1_traindata_Train_BS_Scaled[,-1], \n               y=T1_traindata_Train_BS_Scaled$DLQs,\n               mtryStart = 2,\n               ntreeTry=150, \n               stepFactor = 1, ## 1st try 2 variables, next 4 , next 5 , next 6 MtryStart*Stepfactor \n               improve = 0.001, ## delta OOB \n               trace=TRUE, \n               plot = TRUE,\n               doBest = TRUE,\n               nodesize = 20, \n               importance=TRUE\n) # random Forest tuning also lead to mtry = 2\n\ny_pred = predict(T1_RF, newdata = T1_traindata_Train_BS_Scaled[-1], type='class')\nCM = table(T1_traindata_Train_BS_Scaled[,1],y_pred)\nRF_Speci_Train = CM[4]/(CM[4]+CM[2])\n\nset.seed(1234)\nfolds = createFolds(T1_traindata_Train_BS_Scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = T1_traindata_Train_BS_Scaled[-x, ]\n  test_fold = T1_traindata_Train_BS_Scaled[x, ]\n  T1_RF_temp = randomForest(DLQs ~ ., data = training_fold, \n                            ntree=150, mtry = 2, nodesize = 20)\n  y_pred = predict(T1_RF_temp, newdata = test_fold[-1], type='class')\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nRF_Speci_KF = round(mean(as.numeric(cv)),5)*100\n\ny_pred = predict(T1_RF, newdata = T1_traindata_Test_BS_Scaled[-1], type='class')\nCM = table(T1_traindata_Test_BS_Scaled[,1],y_pred)\nRF_Speci_Test = CM[4]/(CM[4]+CM[2]) # overfitted\n\n# ANN -- Specificity:Train - 83.96 K-fold Train - xxxxxx Test 100 ----\ntraining_set_scaled_ANN = T1_traindata_Train_BS_Scaled\ntraining_set_scaled_ANN$DLQs = as.numeric(as.character(training_set_scaled_ANN$DLQs))\ntest_set_scaled_ANN = T1_traindata_Test_BS_Scaled\ntest_set_scaled_ANN$DLQs = as.numeric(as.character(test_set_scaled_ANN$DLQs))\n\nn = names(training_set_scaled_ANN)\nlong_formula = as.formula(paste(\"DLQs ~\", paste(n[!n %in% \"DLQs\"], collapse = \" + \")))\nset.seed(123)\nT1_ANN = neuralnet(formula = long_formula,\n                   data = training_set_scaled_ANN,\n                   hidden = c(4),\n                   err.fct = \"sse\",\n                   linear.output = FALSE,\n                   lifesign = \"full\",\n                   lifesign.step = 1,\n                   threshold = 0.05,\n                   stepmax = 100000)\nplot(T1_ANN)\ny_pred = ifelse(T1_ANN$net.result[[1]] >= 0.5,1,0)\nCM = table(training_set_scaled_ANN[,1],y_pred)\nANN_Speci_Train = CM[4]/(CM[4]+CM[2])\nANN_Speci_Train\n\ny_pred = compute(T1_ANN,test_set_scaled_ANN[,-1])\ny_pred = ifelse(y_pred$net.result >= 0.5,1,0)\nCM = table(test_set_scaled_ANN[,1],y_pred)\nANN_Speci_Test = CM[4]/(CM[4]+CM[2])\nANN_Speci_Test # underfitted\n\n# Test data prep LR: 49.07 KNN: 71.29 SVM:3.7 NB: 39.81 CART: 25 RF: 67.59 ANN: 36.11 ----\nMissing_data_Check(T1_testdata)\ndata_C = subset(T1_testdata,!is.na(Dependents))\ndata_M = subset(T1_testdata,is.na(Dependents))\ndependents = knn(train = scale(data_C[,c(2,3,4)]),\n                 test = as.matrix(cbind(scale(data_M[2]), data_M[3], scale(data_M[4]))),\n                 cl = as.factor(data_C$Dependents),\n                 k = 9,\n                 prob = F)\ndata_M$Dependents = dependents\nT1_testdata = rbind(data_C,data_M)\nrm(list = c('data_C','data_M'))\nMissing_data_Check(T1_testdata)\nT1_testdata$Dependents = as.numeric(T1_testdata$Dependents)\nTwo_D_View(T1_testdata)\nT1_testdata_BS = SMOTE_fitting(T1_testdata,500,300)\nT1_testdata_BS = T1_testdata_BS[,c(1:5)]\nT1_testdata_BS_Scaled = Scaling(T1_testdata_BS)\n\nT1_traindata_Complete_BS_Scaled = rbind(T1_traindata_Train_BS_Scaled, \n                                        T1_traindata_Test_BS_Scaled)\nT1_CART = rpart(formula = DLQs ~ .,\n                data = T1_traindata_Complete_BS_Scaled,\n                method = \"class\",\n                minsplit= 25,\n                cp = 0.06535947712,\n                xval = 7)\ny_pred = predict(T1_CART, newdata = T1_testdata_BS_Scaled[-1],type='class')\nCM = table(T1_testdata_BS_Scaled[,1],y_pred)\nCART_Speci_Hold = CM[4]/(CM[4]+CM[2]) #25\n\n# T1_LR = glm( formula = DLQs~.,\n#              family = binomial,\n#              data = T1_traindata_Complete_BS_Scaled)\n# prob_pred = predict(T1_LR, type = 'response', newdata = T1_testdata_BS_Scaled[-1])\n# y_pred = ifelse(prob_pred > 0.4, 1, 0)\n# CM = table(T1_testdata_BS_Scaled$DLQs,y_pred)\n# LR_Speci_Hold = CM[4]/(CM[4]+CM[2])\n# round(LR_Speci_Hold*100,2) # 49.07\n# \n# y_pred = knn(train =T1_traindata_Complete_BS_Scaled[,-1],\n#              test =T1_testdata_BS_Scaled[,-1],\n#              cl = T1_traindata_Complete_BS_Scaled[, 1],\n#              k = 5,\n#              prob = TRUE)\n# CM = table(T1_testdata_BS_Scaled[,1],y_pred)\n# Knn_Speci_Hold = CM[4]/(CM[4]+CM[2])\n# round(Knn_Speci_Hold*100,2) # 71.3\n# \n# T1_SVM = svm(formula = DLQs ~ .,\n#              data = T1_traindata_Complete_BS_Scaled,\n#              type = 'C-classification',\n#              kernel = 'radial', cost= 50, gamma= 9)\n# y_pred = predict(T1_SVM, newdata = T1_testdata_BS_Scaled[-1])\n# CM = table(T1_testdata_BS_Scaled[,1],y_pred)\n# SVM_Speci_Hold = CM[4]/(CM[4]+CM[2]) #3.7\n# \n# T1_NB = naiveBayes(x = T1_traindata_Complete_BS_Scaled[-1],\n#                    y = T1_traindata_Complete_BS_Scaled$DLQs)\n# y_pred = predict(T1_NB, newdata = T1_testdata_BS_Scaled[-1])\n# CM = table(T1_testdata_BS_Scaled[,1],y_pred)\n# NB_Speci_Hold = CM[4]/(CM[4]+CM[2]) #39.81\n# \n# \n# T1_RF = randomForest(DLQs ~ ., data = T1_traindata_Complete_BS_Scaled,\n#                      ntree=150, mtry = 2, nodesize = 20,\n#                      importance=TRUE)\n# y_pred = predict(T1_RF, newdata = T1_testdata_BS_Scaled[-1], type='class')\n# CM = table(T1_testdata_BS_Scaled[,1],y_pred)\n# RF_Speci_Hold = CM[4]/(CM[4]+CM[2]) # 67.59\n# \n# training_set_scaled_ANN = T1_traindata_Complete_BS_Scaled\n# training_set_scaled_ANN$DLQs = as.numeric(as.character(training_set_scaled_ANN$DLQs))\n# test_set_scaled_ANN = T1_testdata_BS_Scaled\n# test_set_scaled_ANN$DLQs = as.numeric(as.character(test_set_scaled_ANN$DLQs))\n# \n# n = names(training_set_scaled_ANN)\n# long_formula = as.formula(paste(\"DLQs ~\", paste(n[!n %in% \"DLQs\"], collapse = \" + \")))\n# set.seed(123)\n# T1_ANN = neuralnet(formula = long_formula,\n#                    data = training_set_scaled_ANN,\n#                    hidden = c(4),\n#                    err.fct = \"sse\",\n#                    linear.output = FALSE,\n#                    lifesign = \"full\",\n#                    lifesign.step = 1,\n#                    threshold = 0.05,\n#                    stepmax = 100000)\n# y_pred = compute(T1_ANN,test_set_scaled_ANN[,-1])\n# y_pred = ifelse(y_pred$net.result >= 0.5,1,0)\n# CM = table(test_set_scaled_ANN[,1],y_pred)\n# ANN_Speci_Hold = CM[4]/(CM[4]+CM[2])\n# ANN_Speci_Hold # 36.11\n\n\n\n\n\n\n\n\n",
    "created" : 1510473423636.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "45168614",
    "id" : "F13BB7E6",
    "lastKnownWriteTime" : 1510506025,
    "last_content_update" : 1510506065032,
    "path" : "~/GitHub/MachineLearningExamples/Classification/T1.R",
    "project_path" : "T1.R",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}