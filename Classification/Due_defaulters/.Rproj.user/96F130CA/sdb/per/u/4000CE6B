{
    "collab_server" : "",
    "contents" : "require(readxl) ## read Excel Files\nrequire(dplyr) ## data manupulation\nrequire(usdm) ## VIF \nrequire(ggplot2) ## Visualization\nrequire(caTools) ## split\nrequire(class) ## KNN\nrequire(DMwR) ## SMOTE\nrequire(smotefamily) ## BoderLine SMOTE\nrequire(caret) ## K-Fold, tuning\nrequire(e1071) ## SVM\nrequire(rpart) ## CART - Decision Tree\nrequire(randomForest) ## RandomForest\nrequire(neuralnet) ## ANN\nrequire(gridExtra) ## Multiple plots in single pannel\n\n# reading the data ----\nexcel_sheets(path = 'training.xlsx')\ntraindata = read_excel(path = 'training.xlsx', sheet = 'training')\n\n# rename cols, new features, data type adjustments\ncolnames(traindata) = c('RowID','DLQs','Utlz_UnsecLines','DebtRatio',\n                        'Credit_Loans','Dependents')\ntraindata = traindata %>% \n  dplyr::select(DLQs,Utlz_UnsecLines,DebtRatio,Credit_Loans,Dependents) \ntraindata$UUL_flag = ifelse(traindata$Utlz_UnsecLines>1,1,0)\ntraindata$DR_flag = ifelse(traindata$DebtRatio>1,1,0)\nsapply(traindata,class)\ntraindata$Dependents = ifelse(traindata$Dependents =='NA',NA,traindata$Dependents)\ntraindata$Dependents = as.numeric(traindata$Dependents)\ntraindata[,c(1,6,7)] = data.frame(lapply(traindata[,c(1,6,7)],as.factor))\n\n# spliting into two kinds outliers and normal data\nT1_traindata = subset(traindata, UUL_flag == 1 | DR_flag == 1)\nT2_traindata = subset(traindata, UUL_flag == 0 & DR_flag == 0)\n\n# Missing data check\nMissing_data_Check <- function(data_set){\n  NA_Count = sapply(data_set,function(y) sum(length(which(is.na(y))))) \n  Null_Count = sapply(data_set,function(y) sum(length(which(is.null(y)))))\n  Length0_Count = sapply(data_set,function(y) sum(length(which(length(y)==0))))\n  Empty_Count = sapply(data_set,function(y) sum(length(which(y==''))))\n  Total_NonData = NA_Count+Null_Count+Length0_Count+Empty_Count\n  return( Total_NonData )\n}\nMissing_data_Check(T2_traindata)\nMissing_data_Check(T1_traindata)\n\n# Handling missing data  -----\nvif(data.frame(T2_traindata[,c(2:4)]))\nT2_traindata_C = subset(T2_traindata,!is.na(Dependents))\nT2_traindata_M = subset(T2_traindata,is.na(Dependents))\n\nset.seed(123)\nsplit = sample.split(T2_traindata_C$Dependents, SplitRatio = 0.75)\nT2_traindata_C_Tr = subset(T2_traindata_C, split == TRUE)\nT2_traindata_C_Te = subset(T2_traindata_C, split == FALSE)\ndependents = knn(train = scale(T2_traindata_C_Tr[,c(2,3,4)]),\n                 test = scale(T2_traindata_C_Te[,c(2,3,4)]),\n                 cl = as.factor(T2_traindata_C_Tr$Dependents),\n                 k = 9,\n                 prob = F)\nprint(length(which(T2_traindata_C_Te$Dependents == dependents))/length(dependents))\nmodel = lm(Dependents~Utlz_UnsecLines+DebtRatio+Credit_Loans,\n           data=T2_traindata_C_Tr)\nsummary(model)\ndependents = round(predict(model,T2_traindata_C_Te)) ## LR is not working as it gives all as 1\nrm(list = c('T2_traindata_C_Te','T2_traindata_C_Tr'))\n\ndependents = knn(train = scale(T2_traindata_C[,c(2,3,4)]),\n                 test = scale(T2_traindata_M[,c(2,3,4)]),\n                 cl = as.factor(T2_traindata_C$Dependents),\n                 k = 9,\n                 prob = F)\nT2_traindata_M$Dependents = dependents\nT2_traindata = rbind(T2_traindata_C,T2_traindata_M)\nrm(list = c('T2_traindata_C','T2_traindata_M'))\nMissing_data_Check(T2_traindata)\n\nvif(data.frame(T1_traindata[,c(2:4)]))\nT1_traindata_C = subset(T1_traindata,!is.na(Dependents))\nT1_traindata_M = subset(T1_traindata,is.na(Dependents))\n\nset.seed(123)\nsplit = sample.split(T1_traindata_C$Dependents, SplitRatio = 0.75)\nT1_traindata_C_Tr = subset(T1_traindata_C, split == TRUE)\nT1_traindata_C_Te = subset(T1_traindata_C, split == FALSE)\ndependents = knn(train = scale(T1_traindata_C_Tr[,c(2,3,4)]),\n                 test = scale(T1_traindata_C_Te[,c(2,3,4)]),\n                 cl = as.factor(T1_traindata_C_Tr$Dependents),\n                 k = 9,\n                 prob = F)\nprint(length(which(T1_traindata_C_Te$Dependents == dependents))/length(dependents))\nmodel = lm(Dependents~Utlz_UnsecLines+DebtRatio+Credit_Loans,\n           data=T1_traindata_C_Tr)\nsummary(model)\ndependents = round(predict(model,T1_traindata_C_Te))\nprint(length(which(T1_traindata_C_Te$Dependents == dependents))/length(dependents))\nrm(list = c('T1_traindata_C_Te','T1_traindata_C_Tr'))\n\ndependents = knn(train = scale(T1_traindata_C[,c(2,3,4)]),\n                 test = scale(T1_traindata_M[,c(2,3,4)]),\n                 cl = as.factor(T1_traindata_C$Dependents),\n                 k = 9,\n                 prob = F)\nT1_traindata_M$Dependents = dependents\nT1_traindata = rbind(T1_traindata_C,T1_traindata_M)\nrm(list = c('T1_traindata_C','T1_traindata_M'))\nMissing_data_Check(T1_traindata)\n\n## balanced dataset building for T2 - 94:6 target varibale ratio ----\nT2_traindata = T2_traindata[,1:5]\nT2_traindata$Dependents = as.numeric(T2_traindata$Dependents)\n\nggplot(data = T2_traindata)+\n  geom_point(aes(x = Utlz_UnsecLines, y = DebtRatio,\n                 #shape = as.factor(Dependents), size = Credit_Loans, \n                 color = DLQs))\n# SMOTE for treating imbalance data set 94:6 ratio T- variable\nT2_traindata_SMOTE = SMOTE(DLQs~Utlz_UnsecLines+DebtRatio+\n                             Credit_Loans+Dependents,as.data.frame(T2_traindata),\n                           perc.over = 600,perc.under = 300)\ntable(T2_traindata$DLQs)\ntable(T2_traindata_SMOTE$DLQs)\ntable(T2_traindata_SMOTE$DLQs)/length(T2_traindata_SMOTE$DLQs)\n\nggplot(data = T2_traindata_SMOTE)+\n  geom_point(aes(x = Utlz_UnsecLines, y = DebtRatio,\n                 #shape = as.factor(Dependents), size = Credit_Loans, \n                 color = DLQs))\n\n# SMOTE has oversampled the major class area too - so trying boundary SMOTE\nT2_traindata_SMOTE_B = BLSMOTE(as.data.frame(T2_traindata[2:5]),as.numeric(T2_traindata$DLQs),\n                               K=4,C=3,dupSize=25,method =c(\"type1\"))\ntable(T2_traindata_SMOTE_B$data$class)/length(T2_traindata_SMOTE_B$data$class)\ntable(T2_traindata$DLQs)/length(T2_traindata$DLQs)\nT2_traindata_SMOTE_BS = T2_traindata_SMOTE_B$data\nT2_traindata_SMOTE_BS$DLQs = ifelse(T2_traindata_SMOTE_BS$class == 1, 0, 1)\nT2_traindata_SMOTE_BS = T2_traindata_SMOTE_BS[,c(6,1,2,3,4)]\nT2_traindata_SMOTE_BS$DLQs = as.factor(T2_traindata_SMOTE_BS$DLQs)\n\nggplot(data = T2_traindata_SMOTE_BS)+\n  geom_point(aes(x = Utlz_UnsecLines, y = DebtRatio,\n                 #shape = as.factor(Dependents), size = Credit_Loans, \n                 color = DLQs))\n\nsplit = sample.split(T2_traindata_SMOTE_BS$DLQs, SplitRatio = 0.75)\ntraining_set = subset(T2_traindata_SMOTE_BS, split == TRUE)\ntest_set = subset(T2_traindata_SMOTE_BS, split == FALSE)\nT2_traindata_SMOTE_BS_scaled = T2_traindata_SMOTE_BS\nT2_traindata_SMOTE_BS_scaled[-1] = scale(T2_traindata_SMOTE_BS_scaled[-1])\ntraining_set_scaled = training_set\ntraining_set_scaled[-1] = scale(training_set_scaled[-1])\ntest_set_scaled = test_set\ntest_set_scaled[-1] = scale(test_set_scaled[-1])\n\n# Logistic regression -- Specificity: Train - 62.591 K-fold Train - 63.004 Test - 60.869 ----\nT2_LR = glm( formula = DLQs~., \n             family = binomial,\n             data = training_set)\nprob_pred = predict(T2_LR, type = 'response', newdata = training_set[-1])\ny_pred = ifelse(prob_pred > 0.55, 1, 0)\nCM = table(training_set[,1],y_pred)\nLR_Speci_Train = CM[4]/(CM[4]+CM[2])\nLR_Speci_Train\n\nrequire(lmtest)\nlrtest(T2_LR) # overall test i significant\nrequire(pscl)\npR2(T2_LR) # 35 - very good McFadden R2\n\nset.seed(1234)\nfolds = createFolds(training_set$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = training_set[-x, ]\n  test_fold = training_set[x, ]\n  T2_LR_KF = glm( formula = DLQs~., \n                  family = binomial,\n                  data = training_fold)\n  prob_pred = predict(T2_LR_KF, type = 'response', newdata = test_fold[-1])\n  y_pred = ifelse(prob_pred > 0.55, 1, 0)\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nLR_Speci_KF = mean(as.numeric(cv))\n\nprob_pred = predict(T2_LR, type = 'response', newdata = test_set[-1])\ny_pred = ifelse(prob_pred > 0.55, 1, 0)\nCM = table(test_set[,1],y_pred)\nLR_Speci_Test = CM[4]/(CM[4]+CM[2])\nLR_Speci_Test\n\n# KNN Classification -- Specificity:Train - xxxxx K-fold Train - 87.363 Test 85.714  ---- \ncaret_tune = train(form = DLQs~ ., data = training_set_scaled, method = 'knn')\ncaret_tune\ncaret_tune$bestTune # caret to tune for k value\n\ny_pred = knn(train =training_set_scaled[,-1],\n             test =test_set_scaled[,-1],\n             cl = training_set_scaled[, 1],\n             k = 5,\n             prob = TRUE)\nCM = table(test_set_scaled[,1],y_pred)\nKnn_Speci_Test = CM[4]/(CM[4]+CM[2])\nKnn_Speci_Test\n\nset.seed(1234)\nfolds = createFolds(training_set_scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = training_set_scaled[-x, ]\n  test_fold = training_set_scaled[x, ]\n  y_pred = knn(train =training_fold[,-1],\n               test =test_fold[,-1],\n               cl = training_fold[, 1],\n               k = 5,\n               prob = TRUE)\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nKnn_Speci_KF = mean(as.numeric(cv))\n\n# SVM Classification -- Specificity:Train - 90.881 K-fold Train - 86.116 Test 87.267  ---- \ncaret_tune = train(form = DLQs~ ., data = training_set_scaled, method = 'svmLinearWeights')\ncaret_tune\ncaret_tune$bestTune # caret to tune for cost and weight value - cost is 1 which is default\ntune_svm_kernal = tune(svm, DLQs~ ., data = training_set_scaled,\n                       kernal = 'radial',\n                       ranges = list(cost = c(0.1,0.4,0.8,1,3,5,10,50,100), # penalising factor for missclassification, high c => low bias, high viariance, default is 1\n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4))) # smoothening the boundary shape sharpness, low gama => pointy bounday, low bias, high variance, default 1/dimensions\nsummary(tune_svm_kernal) # tuned parameters says cost 3 and gamma 4\ntune_svm_kernal = tune(svm, DLQs~ ., data = training_set_scaled,\n                       kernal = 'sigmoid',\n                       ranges = list(cost = c(0.1,0.4,0.8,1,3,5,10,50,100), \n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4))) \nsummary(tune_svm_kernal) # tuned parameters says cost 1 and gamma 4\ntune_svm_kernal = tune(svm, DLQs~ ., data = training_set_scaled,\n                       kernal = 'polynomial',\n                       ranges = list(ccost = c(0.1,0.4,0.8,1,3,5,10,50,100),\n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4),\n                                     degree = c(2,3,4,5,6)))\nsummary(tune_svm_kernal) # tuned parameters says cost 0.1 and gamma 4 and degree 3\n\nfor(svmType in c('C-classification','nu-classification')){\n  for(svmKernal in c('linear','polynomial','radial','sigmoid')){\n    set.seed(1234)\n    folds = createFolds(training_set_scaled$DLQs, k = 10)\n    cv = lapply(folds, function(x) {\n      training_fold = training_set_scaled[-x, ]\n      test_fold = training_set_scaled[x, ]\n      if(svmKernal == 'radial'){\n        T2_SVM = svm(formula = DLQs ~ .,\n                     data = training_fold,\n                     type = 'C-classification',\n                     kernel = svmKernal, cost = 3,gamma = 4)\n        y_pred = predict(T2_SVM, newdata = test_fold[-1])\n      }else if(svmKernal=='sigmoid'){\n        T2_SVM = svm(formula = DLQs ~ .,\n                     data = training_fold,\n                     type = 'C-classification',\n                     kernel = svmKernal, cost = 1,gamma = 4)\n        y_pred = predict(T2_SVM, newdata = test_fold[-1])\n      }else{\n        T2_SVM = svm(formula = DLQs ~ .,\n                     data = training_fold,\n                     type = 'C-classification',\n                     kernel = svmKernal)\n        y_pred = predict(T2_SVM, newdata = test_fold[-1])\n      }\n      CM = table(test_fold[,1],y_pred)\n      temp = CM[4]/(CM[4]+CM[2])\n      return(temp)\n    })\n    specificity_SVM = round(mean(as.numeric(cv)),5)*100\n    print.noquote(paste0(svmKernal,'-kernal ',svmType,' has K-fold specificity of ',specificity_SVM))\n  }\n} # choose radial kernal with C-Classification as it has highest 86.116\n\n# [1] linear-kernal C-classification has K-fold specificity of 80.112\n# [1] polynomial-kernal C-classification has K-fold specificity of 60.208\n# [1] radial-kernal C-classification has K-fold specificity of 86.116\n# [1] sigmoid-kernal C-classification has K-fold specificity of 40.947\n# [1] linear-kernal nu-classification has K-fold specificity of 80.112\n# [1] polynomial-kernal nu-classification has K-fold specificity of 60.208\n# [1] radial-kernal nu-classification has K-fold specificity of 86.116\n# [1] sigmoid-kernal nu-classification has K-fold specificity of 40.947\nT2_SVM = svm(formula = DLQs ~ .,\n             data = training_set_scaled,\n             type = 'C-classification',\n             kernel = 'radial', cost= 3, gamma= 4)\ny_pred = predict(T2_SVM, newdata = training_set_scaled[-1])\nCM = table(training_set_scaled[,1],y_pred)\nSVM_Speci_Train = CM[4]/(CM[4]+CM[2])\n\ny_pred = predict(T2_SVM, newdata = test_set_scaled[-1])\nCM = table(test_set_scaled[,1],y_pred)\nSVM_Speci_Test = CM[4]/(CM[4]+CM[2])\n\n# Naive Bayes -- Specificity:Train - 81.347 K-fold Train - 81.557 Test 83.851   ----\nT2_NB = naiveBayes(x = training_set[-1],\n                   y = training_set_scaled$DLQs)\n\ny_pred = predict(T2_NB, newdata = training_set_scaled[-1])\nCM = table(training_set_scaled[,1],y_pred)\nNB_Speci_Train = CM[4]/(CM[4]+CM[2])\n\nset.seed(1234)\nfolds = createFolds(training_set_scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = training_set_scaled[-x, ]\n  test_fold = training_set_scaled[x, ]\n  T2_NB = naiveBayes(x = training_fold[-1],\n                     y = training_fold$DLQs)\n  y_pred = predict(T2_SVM, newdata = test_fold[-1])\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nNB_Speci_KF = round(mean(as.numeric(cv)),5)*100\n\ny_pred = predict(T2_NB, newdata = test_set_scaled[-1])\nCM = table(test_set_scaled[,1],y_pred)\nNB_Speci_Test = CM[4]/(CM[4]+CM[2])\n\n# CART -- Specificity:Train - 71.813 K-fold Train - 75.866 Test 72.360    --------\ncaret_tune = train(form = DLQs~ ., data = training_set_scaled, method = 'rpart')\ncaret_tune\ncaret_tune$bestTune # CP - Tunning \n\nT2_CART_temp = rpart(formula = DLQs ~ ., \n                     data = training_set_scaled, \n                     method = \"class\", \n                     minsplit= 225, \n                     cp = 0, \n                     xval = 7)\nprintcp(T2_CART_temp)\nplotcp(T2_CART_temp)\nT2_CART = prune(T2_CART_temp, cp= 0.05284974 ,\"CP\")\ny_pred = predict(T2_CART, newdata = training_set_scaled[-1], type='class')\nCM = table(training_set_scaled[,1],y_pred)\nCART_Speci_Train = CM[4]/(CM[4]+CM[2])\n\nset.seed(1234)\nfolds = createFolds(training_set_scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = training_set_scaled[-x, ]\n  test_fold = training_set_scaled[x, ]\n  T2_CART_temp = rpart(formula = DLQs ~ ., \n                       data = training_fold, \n                       method = \"class\", \n                       minsplit= 225, \n                       cp = 0.05284974, \n                       xval = 7)\n  y_pred = predict(T2_CART_temp, newdata = test_fold[-1], type='class')\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nCART_Speci_KF = round(mean(as.numeric(cv)),5)*100\n\ny_pred = predict(T2_CART, newdata = test_set_scaled[-1],type='class')\nCM = table(test_set_scaled[,1],y_pred)\nCART_Speci_Test = CM[4]/(CM[4]+CM[2])\n\n# Random Forest -- Specificity:Train - 86.736 K-fold Train - 82.279 Test 83.851 ------\nset.seed(1234)\nT2_RF = randomForest(DLQs ~ ., data = training_set_scaled, \n                     ntree=500, mtry = 2, nodesize = 40,\n                     importance=TRUE)\nplot(T2_RF) ## 250 tree from OOB\ncaret_tune = train(form = DLQs~ ., data = training_set_scaled, method = 'rf')\ncaret_tune\ncaret_tune$bestTune # mtry - Tunning \n\nset.seed(1234)\nT2_RF = tuneRF(x = training_set_scaled[,-1], \n               y=training_set_scaled$DLQs,\n               mtryStart = 2,\n               ntreeTry=250, \n               stepFactor = 1, ## 1st try 3 variables, next 6 , next 12 , next 24 MtryStart*Stepfactor \n               improve = 0.001, ## delta OOB \n               trace=TRUE, \n               plot = TRUE,\n               doBest = TRUE,\n               nodesize = 20, \n               importance=TRUE\n) # random Forest tuning also lead to mtry = 2\n\ny_pred = predict(T2_RF, newdata = training_set_scaled[-1], type='class')\nCM = table(training_set_scaled[,1],y_pred)\nRF_Speci_Train = CM[4]/(CM[4]+CM[2])\n\nset.seed(1234)\nfolds = createFolds(training_set_scaled$DLQs, k = 10)\ncv = lapply(folds, function(x) {\n  training_fold = training_set_scaled[-x, ]\n  test_fold = training_set_scaled[x, ]\n  T2_RF_temp = randomForest(DLQs ~ ., data = training_fold, \n                            ntree=500, mtry = 2, nodesize = 40)\n  y_pred = predict(T2_RF_temp, newdata = test_fold[-1], type='class')\n  CM = table(test_fold[,1],y_pred)\n  temp = CM[4]/(CM[4]+CM[2])\n  return(temp)\n})\nRF_Speci_KF = round(mean(as.numeric(cv)),5)*100\n\ny_pred = predict(T2_RF, newdata = test_set_scaled[-1], type='class')\nCM = table(test_set_scaled[,1],y_pred)\nRF_Speci_Test = CM[4]/(CM[4]+CM[2])\n\n# ANN -- Specificity:Train - 87.254 K-fold Train - xxxxxx Test 85.093 ----\ntraining_set_scaled_ANN = training_set_scaled\ntraining_set_scaled_ANN$DLQs = as.numeric(as.character(training_set_scaled_ANN$DLQs))\ntest_set_scaled_ANN = test_set_scaled\ntest_set_scaled_ANN$DLQs = as.numeric(as.character(test_set_scaled_ANN$DLQs))\n\nn = names(training_set_scaled_ANN)\nlong_formula = as.formula(paste(\"DLQs ~\", paste(n[!n %in% \"DLQs\"], collapse = \" + \")))\nset.seed(123)\nT2_ANN = neuralnet(formula = long_formula,\n                   data = training_set_scaled_ANN,\n                   hidden = c(5,5),\n                   err.fct = \"sse\",\n                   linear.output = FALSE,\n                   lifesign = \"full\",\n                   lifesign.step = 1,\n                   threshold = 0.05,\n                   stepmax = 100000)\nplot(T2_ANN)\ny_pred = ifelse(T2_ANN$net.result[[1]] >= 0.5,1,0)\nCM = table(training_set_scaled_ANN[,1],y_pred)\nANN_Speci_Train = CM[4]/(CM[4]+CM[2])\nANN_Speci_Train\n\ny_pred = compute(T2_ANN,test_set_scaled_ANN[,-1])\ny_pred = ifelse(y_pred$net.result >= 0.5,1,0)\nCM = table(test_set_scaled_ANN[,1],y_pred)\nANN_Speci_Test = CM[4]/(CM[4]+CM[2])\nANN_Speci_Test\n\n# Test Data preparation-----\n# reading the data\nexcel_sheets(path = 'test.xlsx')\ntestdata = read_excel(path = 'test.xlsx', sheet = 'test')\n\n# rename cols, new features, data type adjustments\ncolnames(testdata) = c('RowID','DLQs','Utlz_UnsecLines','DebtRatio',\n                       'Credit_Loans','Dependents')\ntestdata = testdata %>% \n  dplyr::select(DLQs,Utlz_UnsecLines,DebtRatio,Credit_Loans,Dependents) \ntestdata$UUL_flag = ifelse(testdata$Utlz_UnsecLines>1,1,0)\ntestdata$DR_flag = ifelse(testdata$DebtRatio>1,1,0)\nsapply(testdata,class)\ntestdata$Dependents = ifelse(testdata$Dependents =='NA',NA,testdata$Dependents)\ntestdata$Dependents = as.numeric(testdata$Dependents)\ntestdata[,c(1,6,7)] = data.frame(lapply(testdata[,c(1,6,7)],as.factor))\n\n# spliting into two kinds outliers and normal data\nT1_testdata = subset(testdata, UUL_flag == 1 | DR_flag == 1)\nT2_testdata = subset(testdata, UUL_flag == 0 & DR_flag == 0)\n\n# Check missing values\nMissing_data_Check(T2_testdata)\nMissing_data_Check(T1_testdata)\n\n# Handling missing data \nvif(data.frame(T2_testdata[,c(2:4)]))\nT2_testdata_C = subset(T2_testdata,!is.na(Dependents))\nT2_testdata_M = subset(T2_testdata,is.na(Dependents))\n\ndependents = knn(train = scale(T2_testdata_C[,c(2,3,4)]),\n                 test = as.matrix(cbind(scale(T2_testdata_M[2]), T2_testdata_M[3], scale(T2_testdata_M[4]))),\n                 cl = as.factor(T2_testdata_C$Dependents),\n                 k = 3,\n                 prob = F)\nT2_testdata_M$Dependents = dependents\nT2_testdata = rbind(T2_testdata_C,T2_testdata_M)\nrm(list = c('T2_testdata_C','T2_testdata_M'))\nMissing_data_Check(T2_testdata)\n\nvif(data.frame(T1_testdata[,c(2:4)]))\nT1_testdata_C = subset(T1_testdata,!is.na(Dependents))\nT1_testdata_M = subset(T1_testdata,is.na(Dependents))\n\ndependents = knn(train = scale(T1_testdata_C[,c(2,3,4)]),\n                 test = as.matrix(cbind(scale(T1_testdata_M[2]), T1_testdata_M[3], scale(T1_testdata_M[4]))),\n                 cl = as.factor(T1_testdata_C$Dependents),\n                 k = 9,\n                 prob = F)\nT1_testdata_M$Dependents = dependents\nT1_testdata = rbind(T1_testdata_C,T1_testdata_M)\nrm(list = c('T1_testdata_C','T1_testdata_M'))\nMissing_data_Check(T1_testdata)\n\n## Model building for T2 - 94:6 target varibale ratio\nT2_testdata = T2_testdata[,1:5]\nT2_testdata$Dependents = as.numeric(T2_testdata$Dependents)\n\nggplot(data = T2_testdata)+\n  geom_point(aes(x = Utlz_UnsecLines, y = DebtRatio,\n                 #shape = as.factor(Dependents), size = Credit_Loans, \n                 color = DLQs))\n# SMOTE has oversampled the major class area too - so trying boundary SMOTE\nT2_testdata_SMOTE_B = BLSMOTE(as.data.frame(T2_testdata[2:5]),as.numeric(T2_testdata$DLQs),\n                              K=4,C=3,dupSize=15,method =c(\"type1\"))\ntable(T2_testdata_SMOTE_B$data$class)/length(T2_testdata_SMOTE_B$data$class)\ntable(T2_testdata$DLQs)/length(T2_testdata$DLQs)\nT2_testdata_SMOTE_BS = T2_testdata_SMOTE_B$data\nT2_testdata_SMOTE_BS$DLQs = ifelse(T2_testdata_SMOTE_BS$class == 1, 0, 1)\nT2_testdata_SMOTE_BS = T2_testdata_SMOTE_BS[,c(6,1,2,3,4)]\nT2_testdata_SMOTE_BS$DLQs = as.factor(T2_testdata_SMOTE_BS$DLQs)\n\nggplot(data = T2_testdata_SMOTE_BS)+\n  geom_point(aes(x = Utlz_UnsecLines, y = DebtRatio,\n                 #shape = as.factor(Dependents), size = Credit_Loans, \n                 color = DLQs))\n\nT2_testdata_Scale = T2_testdata_SMOTE_BS\nT2_testdata_Scale[,-1] = scale(T2_testdata_Scale[,-1]) \n\n# Model Testing againist Test data LR: 48.889 KNN: 24.444 SVM: 19.0 NB: 75.667 CART: 84.667 RF: 25.333 ANN: 51.0  ----\n\nT2_LR = glm( formula = DLQs~., \n             family = binomial,\n             data = T2_traindata_SMOTE_BS_scaled)\nprob_pred = predict(T2_LR, type = 'response', newdata = T2_testdata_Scale[-1])\ny_pred = ifelse(prob_pred > 0.55, 1, 0)\nCM = table(T2_testdata_Scale$DLQs,y_pred)\nLR_Speci_Hold = CM[4]/(CM[4]+CM[2])\nLR_Speci_Hold \n\ny_pred = knn(train =T2_traindata_SMOTE_BS_scaled[,-1],\n             test =T2_testdata_Scale[,-1],\n             cl = T2_traindata_SMOTE_BS_scaled[, 1],\n             k = 5,\n             prob = TRUE)\nCM = table(T2_testdata_Scale$DLQs,y_pred)\nKnn_Speci_Hold = CM[4]/(CM[4]+CM[2])\nKnn_Speci_Hold\n\nT2_SVM = svm(formula = DLQs ~ .,\n             data = T2_traindata_SMOTE_BS_scaled,\n             type = 'C-classification',\n             kernel = 'radial', cost= 3, gamma= 4)\ny_pred = predict(T2_SVM, newdata = T2_testdata_Scale[-1])\nCM = table(T2_testdata_Scale$DLQs,y_pred)\nSVM_Speci_Hold = CM[4]/(CM[4]+CM[2])\nSVM_Speci_Hold\n\nT2_NB = naiveBayes(x = T2_traindata_SMOTE_BS_scaled[-1],\n                   y = T2_traindata_SMOTE_BS_scaled$DLQs)\n\ny_pred = predict(T2_NB, newdata = T2_testdata_Scale[-1])\nCM = table(T2_testdata_Scale$DLQs,y_pred)\nNB_Speci_Hold = CM[4]/(CM[4]+CM[2])\nNB_Speci_Hold\n\nT2_CART = rpart(formula = DLQs ~ ., \n                data = T2_traindata_SMOTE_BS_scaled, \n                method = \"class\", \n                minsplit= 225, \n                cp = 0.05284974, \n                xval = 7)\ny_pred = predict(T2_CART, newdata = T2_testdata_Scale[-1], type='class')\nCM = table(T2_testdata_Scale$DLQs,y_pred)\nCART_Speci_Hold = CM[4]/(CM[4]+CM[2])\nCART_Speci_Hold\n\nT2_RF = randomForest(DLQs ~ ., data = T2_traindata_SMOTE_BS_scaled, \n                     ntree=250, mtry = 2, nodesize = 40,\n                     importance=TRUE)\ny_pred = predict(T2_RF, newdata = T2_testdata_Scale[-1], type='class')\nCM = table(T2_testdata_Scale$DLQs,y_pred)\nRF_Speci_Hold = CM[4]/(CM[4]+CM[2])\nRF_Speci_Hold\n\nT2_traindata_SMOTE_BS_scaled_ANN = T2_traindata_SMOTE_BS_scaled\nT2_traindata_SMOTE_BS_scaled_ANN$DLQs = as.numeric(as.character(T2_traindata_SMOTE_BS_scaled_ANN$DLQs))\nT2_testdata_Scale_ANN = T2_testdata_Scale\nT2_testdata_Scale_ANN$DLQs = as.numeric(as.character(T2_testdata_Scale_ANN$DLQs))\nT2_ANN = neuralnet(formula = long_formula,\n                   data = training_set_scaled_ANN,\n                   hidden = c(5,5),\n                   err.fct = \"sse\",\n                   linear.output = FALSE,\n                   lifesign = \"full\",\n                   lifesign.step = 1,\n                   threshold = 0.05,\n                   stepmax = 100000)\nplot(T2_ANN)\ny_pred = compute(T2_ANN,T2_testdata_Scale[,-1])\ny_pred = ifelse(y_pred$net.result >= 0.5,1,0)\nCM = table(T2_testdata_Scale_ANN$DLQs,y_pred)\nANN_Speci_Hold = CM[4]/(CM[4]+CM[2])\nANN_Speci_Hold\n\n## Training data -- NON Ouliers Visuals in 2D ------\nP1 = ggplot(data = T2_traindata_SMOTE_BS)+\n  geom_point(aes(x = Utlz_UnsecLines, y = DebtRatio,\n                 color = DLQs),show.legend = F)\nP2 = ggplot(data = T2_traindata_SMOTE_BS)+\n  geom_point(aes(x = Dependents, y = Credit_Loans,\n                 color = DLQs),show.legend = F)\nP3 = ggplot(data = T2_traindata_SMOTE_BS)+\n  geom_point(aes(x = Utlz_UnsecLines, y = Credit_Loans,\n                 color = DLQs),show.legend = F)\nP4 = ggplot(data = T2_traindata_SMOTE_BS)+\n  geom_point(aes(x = Dependents, y = DebtRatio,\n                 color = DLQs),show.legend = F)\ngrid.arrange(P1, P2, P3,P4, ncol = 2, nrow = 2)\n\n## Test data \nT1 = ggplot(data = T2_testdata_SMOTE_BS)+\n  geom_point(aes(x = Utlz_UnsecLines, y = DebtRatio,\n                 color = DLQs),show.legend = F)\nT2 = ggplot(data = T2_testdata_SMOTE_BS)+\n  geom_point(aes(x = Dependents, y = Credit_Loans,\n                 color = DLQs),show.legend = F)\nT3 = ggplot(data = T2_testdata_SMOTE_BS)+\n  geom_point(aes(x = Utlz_UnsecLines, y = Credit_Loans,\n                 color = DLQs),show.legend = F)\nT4 = ggplot(data = T2_testdata_SMOTE_BS)+\n  geom_point(aes(x = Dependents, y = DebtRatio,\n                 color = DLQs),show.legend = F)\n\ngrid.arrange(T1, T2, T3,T4, ncol = 2, nrow = 2)\n\ngrid.arrange(P1, P2, P3,P4,T1, T2, T3,T4, ncol = 4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1510353300179.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "1004079105",
    "id" : "4000CE6B",
    "lastKnownWriteTime" : 32370120545140837,
    "last_content_update" : 1510353313537,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}