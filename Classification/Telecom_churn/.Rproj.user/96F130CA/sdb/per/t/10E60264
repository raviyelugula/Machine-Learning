{
    "collab_server" : "",
    "contents" : "require(dplyr)\nrequire(ggplot2)\nrequire(readxl)\nrequire(dummies)\nrequire(randomForest)\nrequire(caTools)\nrequire(caret)\nrequire(usdm)\nrequire(caret)\nrequire(rpart)\nrequire(e1071)\nexcel_sheets('Telco Churn.xlsx')\nrawdata = read_excel('Telco Churn.xlsx','WA_Fn-UseC_-Telco-Customer-Chur')\nsapply(rawdata,class)\ndata_factors = rawdata\nfactor_col_names = c(\"gender\",\"SeniorCitizen\",\"Partner\",\"Dependents\"\n                     ,\"PhoneService\",\"MultipleLines\",\"InternetService\" \n                     ,\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\"\n                     ,\"TechSupport\",\"StreamingTV\",\"StreamingMovies\" \n                     ,\"Contract\",\"PaperlessBilling\",\"PaymentMethod\",\"Churn\")\ndata_factors[factor_col_names] = lapply(data_factors[factor_col_names],factor)\nstr(data_factors)\n\n# for( i in factor_col_names[1:16]){\n#   print(ggplot(data_factors,\n#                 aes(x=i,\n#                     fill =Churn))+\n#           xlab(names(data_factors[i]))+\n#           ylab('Frequency')+\n#           geom_bar(position = 'dodge')+\n#           guides(fill = guide_legend('Churn')))\n# }\n\nMissing_data_Check <- function(data_set){\n  NA_Count = sapply(data_set,function(y) sum(length(which(is.na(y))))) \n  Null_Count = sapply(data_set,function(y) sum(length(which(is.null(y)))))\n  Length0_Count = sapply(data_set,function(y) sum(length(which(length(y)==0))))\n  Empty_Count = sapply(data_set,function(y) sum(length(which(y==''))))\n  Total_NonData = NA_Count+Null_Count+Length0_Count+Empty_Count\n  return( Total_NonData )\n}\nMissing_data_Check(data_factors)\ndata_factors[which(is.na(data_factors$TotalCharges)),'TotalCharges'] = mean(data_factors$TotalCharges,na.rm = T)\ndata_factors = data_factors[-1]\nsapply(data_factors,class)\n# data_factors$Churn = as.factor(ifelse(data_factors$Churn=='Yes',1,0))\n\nset.seed(123)\nsplit = sample.split(data_factors$Churn, SplitRatio = 0.75)\ndata_factors_tr = subset(data_factors, split == TRUE)\ndata_factors_te = subset(data_factors, split == FALSE)\n\nmtry_opt_value = train(form = Churn~ .,\n                       data = data_factors_tr, method = 'rf')\nRF_model = randomForest(Churn ~ ., \n                        data = data_factors_tr, \n                        ntree=200, mtry = 2, nodesize = 20,\n                        importance=TRUE)\nfeatureImp_df = RF_model$importance\n\ncaret_tune = train(form = Churn ~ ., data = data_factors_tr, method = 'rpart')\ncaret_tune\ncaret_tune$bestTune # CP - Tunning \ncart_model = rpart(Churn ~ ., \n                   data = data_factors_tr,\n                   method= 'class',\n                   minsplit = 100,\n                   cp=0.008321446)\ncart_model$variable.importance\n\nfeatureImp_df[order(featureImp_df[,4]),]\nsort(cart_model$variable.importance)\n\nnames(data_factors_tr[,c('tenure','TotalCharges','Contract','InternetService',\n                         'MonthlyCharges','TechSupport','OnlineSecurity','Churn')])\ndata_factors_tr_imp = data_factors_tr[,c('tenure','TotalCharges','Contract','InternetService',\n                                         'MonthlyCharges','TechSupport','OnlineSecurity','Churn')]\ntemp = data_factors_tr[,c('tenure','TotalCharges','Contract','InternetService',\n                          'MonthlyCharges','TechSupport','OnlineSecurity')]\ndummy_df = dummy.data.frame(as.data.frame(temp))\ndummy_df = cbind(dummy_df,Churn=data_factors_tr$Churn)\nvif(dummy_df[,c(3:4)])\nvif(dummy_df[,c(6,7)])\nvif(dummy_df[,c(10:11)])\nvif(dummy_df[,c(13:14)])\nvif(dummy_df[,c(1:4,6,7,9:11,13,14)])\nvif(dummy_df[,c(1,2,9)])\n\ndummy_df = dummy_df[-2]\n\n# log_model_imp= glm(formula = Churn~.,  \n#                    family = binomial,\n#                    data = dummy_df)\n# summary(log_model_imp)\nlog_model_imp= glm(formula = Churn~tenure+Contract+InternetService\n                    +MonthlyCharges+TechSupport+OnlineSecurity,\n                   family = binomial,\n                   data = data_factors_tr[,c('tenure','Contract','InternetService',\n                                             'MonthlyCharges','TechSupport','OnlineSecurity','Churn')])\nsummary(log_model_imp)\nprob_pred = predict(log_model_imp, type = 'response',\n                    newdata = data_factors_te[,c('tenure','Contract','InternetService',\n                                                  'MonthlyCharges','TechSupport','OnlineSecurity')])\ny_pred = ifelse(prob_pred > 0.50, 1, 0)\nCM = table(data_factors_te$Churn,y_pred)\nlog_model_imp_spec = CM[4]/(CM[4]+CM[2])\nround(log_model_imp_spec*100,2)\n\nset.seed(1234)\ncaret_tune = train(form = Churn~ ., data = data_factors_tr_imp, method = 'knn')\ncaret_tune\ncaret_tune$bestTune # caret to tune for k value\n\ny_pred = knn(train =data_factors_tr_imp[,-7],\n             test =data_factors_te[,-20],\n             cl = data_factors_tr_imp$Churn,\n             k = 9,\n             prob = TRUE)\nCM = table(T2_traindata_Test_BS_Scaled[,1],y_pred)\nKnn_Speci_Test = CM[4]/(CM[4]+CM[2])\nKnn_Speci_Test\n\n##################### Naive Bayes all features\n\nNB_model = naiveBayes(x = data_factors_tr[-20],\n                   y = data_factors_tr$Churn)\n\ny_pred = predict(NB_model, newdata = data_factors_te[-20])\nCM = table(data_factors_te$Churn,y_pred)\nNB_Specificity = CM[4]/(CM[4]+CM[2])\nNB_Acc = (CM[1]+CM[4])/(CM[1]+CM[4]+CM[2]+CM[3])\n\n#################### Naive Bayes imp features\n\nNB_model = naiveBayes(x = data_factors_tr[,c('tenure','Contract','InternetService',\n                                             'MonthlyCharges','TechSupport','OnlineSecurity')],\n                      y = data_factors_tr$Churn)\n\ny_pred = predict(NB_model, newdata = data_factors_te[,c('tenure','Contract','InternetService',\n                                                        'MonthlyCharges','TechSupport','OnlineSecurity')])\nCM = table(data_factors_te$Churn,y_pred)\nNB_Specificity_imp = CM[4]/(CM[4]+CM[2])\nNB_Acc_imp = (CM[1]+CM[4])/(CM[1]+CM[4]+CM[2]+CM[3])\n\n################### RF all features\nRF_model = randomForest(Churn ~ ., \n                        data = data_factors_tr, \n                        ntree=200, mtry = 2, nodesize = 20,\n                        importance=TRUE) # mtry 2 is optimal value - found from caret \ny_pred = predict(RF_model, newdata = data_factors_te[-20], type='class')\nCM = table(data_factors_te$Churn,y_pred)\nRF_Specificity = CM[4]/(CM[4]+CM[2])\nRF_Acc = (CM[1]+CM[4])/(CM[1]+CM[4]+CM[2]+CM[3])\n################# RF Imp features\nRF_model = randomForest(Churn ~ ., \n                        data = data_factors_tr[,c('tenure','Contract','InternetService',\n                                                  'MonthlyCharges','TechSupport','OnlineSecurity','Churn')], \n                        ntree=200, mtry = 2, nodesize = 20,\n                        importance=TRUE) # mtry 2 is optimal value - found from caret \ny_pred = predict(RF_model, newdata = data_factors_te[,c('tenure','Contract','InternetService',\n                                                        'MonthlyCharges','TechSupport','OnlineSecurity')], type='class')\nCM = table(data_factors_te$Churn,y_pred)\nRF_Specificity_imp = CM[4]/(CM[4]+CM[2])\nRF_Acc_imp = (CM[1]+CM[4])/(CM[1]+CM[4]+CM[2]+CM[3])\n\n################## SVM radial all features\ntune_svm_kernal = tune(svm, Churn~ ., data = data_factors_tr,\n                       kernal = 'radial',\n                       ranges = list(cost = c(0.1,0.4,0.8,1,3,5,10,50,100), # penalising factor for missclassification, high c => low bias, high viariance, default is 1\n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4))) # smoothening the boundary shape sharpness, low gama => pointy bounday, low bias, high variance, default 1/dimensions\n\nSVM_model = svm(formula = Churn ~ .,\n             data = data_factors_tr,\n             type = 'C-classification',\n             kernel = 'radial', cost= c, gamma= g)\ny_pred = predict(SVM_model, newdata = data_factors_te[-20])\nCM = table(data_factors_te$Churn,y_pred)\nSVM_Specificity = CM[4]/(CM[4]+CM[2])\nSVM_Acc = (CM[1]+CM[4])/(CM[1]+CM[4]+CM[2]+CM[3])\n\n######### SVM imp features \ntune_svm_kernal = tune(svm, Churn~ ., data = data_factors_tr[,c('tenure','Contract','InternetService',\n                                                                'MonthlyCharges','TechSupport','OnlineSecurity','Churn')],\n                       kernal = 'radial',\n                       ranges = list(cost = c(0.1,0.4,0.8,1,3,5,10,50,100), # penalising factor for missclassification, high c => low bias, high viariance, default is 1\n                                     gamma = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,4))) # smoothening the boundary shape sharpness, low gama => pointy bounday, low bias, high variance, default 1/dimensions\n\nSVM_model = svm(formula = Churn ~ .,\n                data = data_factors_tr[,c('tenure','Contract','InternetService',\n                                          'MonthlyCharges','TechSupport','OnlineSecurity','Churn')],\n                type = 'C-classification',\n                kernel = 'radial', cost= c, gamma= g)\ny_pred = predict(SVM_model, newdata = data_factors_te[,c('tenure','Contract','InternetService',\n                                                         'MonthlyCharges','TechSupport','OnlineSecurity')])\nCM = table(data_factors_te$Churn,y_pred)\nSVM_Specificity_imp = CM[4]/(CM[4]+CM[2])\nSVM_Acc_imp = (CM[1]+CM[4])/(CM[1]+CM[4]+CM[2]+CM[3])\n\n\n\n\n\n",
    "created" : 1511458647385.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4256877795",
    "id" : "10E60264",
    "lastKnownWriteTime" : 1511694054,
    "last_content_update" : 1511694054927,
    "path" : "~/GitHub/MachineLearningExamples/Classification/Telecom_churn/Telco_Churn.R",
    "project_path" : "Telco_Churn.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}